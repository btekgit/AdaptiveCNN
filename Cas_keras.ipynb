{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cas-keras",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/btekgit/AdaptiveCNN/blob/master/Cas_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o1HajCaf20e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c47fc3d-7bc1-4e7c-83f8-a88a5556b475"
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras.utils import conv_utils\n",
        "from keras import activations, regularizers, constraints\n",
        "from keras import initializers\n",
        "from keras.engine import InputSpec\n",
        "import numpy as np\n",
        "#import tensorflow as tf\n",
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "\n",
        "\n",
        "def idx_init(shape, dtype='float32'):\n",
        "    idxs = np.zeros((shape[0], shape[1]),dtype)\n",
        "    c = 0\n",
        "    # assumes square filters\n",
        "    \n",
        "    wid = np.int(np.sqrt(shape[0]))\n",
        "    hei =np.int(np.sqrt(shape[0]))\n",
        "    f = np.float32\n",
        "    for x in np.arange(wid):  # / (self.incoming_width * 1.0):\n",
        "        for y in np.arange(hei):  # / (self.incoming_height * 1.0):\n",
        "            idxs[c, :] = np.array([x/f(wid-1), y/f(hei-1)],dtype)\n",
        "            c += 1\n",
        "\n",
        "    return idxs\n",
        "\n",
        "def cov_init(shape, dtype='float32'):\n",
        "    \n",
        "    cov = np.identity(shape[1], dtype)\n",
        "    # shape [0] must have self.incoming_channels * self.num_filters\n",
        "    cov = np.repeat(cov[np.newaxis], shape[0], axis=0)\n",
        "    \n",
        "    #for t in range(shape[0]):\n",
        "    #    cov[t] = cov[t]\n",
        "    return cov\n",
        "\n",
        "def sigma_init(shape, initsigma, dtype='float32'):\n",
        "        \n",
        "   if isinstance(initsigma,float):  #initialize it with the given scalar\n",
        "       sigma = initsigma*np.ones(shape[0],dtype='float32')\n",
        "   elif isinstance(initsigma,tuple) and len(initsigma)==2: #linspace in range\n",
        "       sigma = np.linspace(initsigma[0], initsigma[1], shape[0],dtype=dtype)\n",
        "   elif isinstance(initsigma,np.ndarray): # set the values directly from array\n",
        "       sigma = (initsigma).astype(dtype=dtype)\n",
        "   else:\n",
        "       print(\"Default initial sigma value 0.1 will be used\")\n",
        "       sigma = np.float32(0.1)*np.ones(shape[0],dtype=dtype)\n",
        "\n",
        "   print(\"Scale initializer:\",sigma)\n",
        "   return sigma.astype(dtype)\n",
        "\n",
        "\n",
        "\n",
        "class Conv2DAdaptive(Layer):\n",
        "    def __init__(self, rank, nfilters,\n",
        "                 kernel_size,\n",
        "                 strides=1,\n",
        "                 padding='valid',\n",
        "                 output_padding=None,\n",
        "                 data_format=None,\n",
        "                 dilation_rate=1,\n",
        "                 activation=None,\n",
        "                 use_bias=False,\n",
        "                 kernel_regularizer=None,\n",
        "                 gain=1.0,\n",
        "                 init_sigma=0.1,\n",
        "                 init_w=initializers.glorot_uniform(),\n",
        "                 init_bias = initializers.Constant(),\n",
        "                 trainsigmas=True,\n",
        "                 trainWeights=True,\n",
        "                 reg_bias=None,\n",
        "                 **kwargs):\n",
        "        super(Conv2DAdaptive, self).__init__(**kwargs)\n",
        "        #def __init__(self, num_filters, kernel_sigmaze, incoming_channels=1, **kwargs):\n",
        "        self.rank = rank\n",
        "        self.nfilters = nfilters\n",
        "        self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')\n",
        "        self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n",
        "        self.padding = conv_utils.normalize_padding(padding)\n",
        "        self.data_format = data_format\n",
        "        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, rank, 'dilation_rate')\n",
        "        self.activation = activations.get(activation)\n",
        "        self.use_bias = use_bias\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.input_spec = InputSpec(ndim=self.rank + 2)\n",
        "        self.gain = gain     \n",
        "        self.initsigma=init_sigma\n",
        "        self.initW =init_w\n",
        "        self.trainsigmas = trainsigmas\n",
        "        self.trainWeights = trainWeights\n",
        "        self.bias_initializer =init_bias\n",
        "        self.bias_regularizer = reg_bias\n",
        "        self.bias_constraint = None\n",
        "        self.sigma =None\n",
        "                 \n",
        "        #self.input_shape = input_shape\n",
        "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
        "            kwargs['input_shape'] = (kwargs.pop('input_dim'))\n",
        "        print(kwargs)\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        self.num_filters = nfilters\n",
        "        #self.incoming_channels = incoming_channels\n",
        "        \n",
        "        \n",
        "        self.output_padding = output_padding\n",
        "        if self.output_padding is not None:\n",
        "            self.output_padding = conv_utils.normalize_tuple(\n",
        "                self.output_padding, 2, 'output_padding')\n",
        "            for stride, out_pad in zip(self.strides, self.output_padding):\n",
        "                if out_pad >= stride:\n",
        "                    raise ValueError('Stride ' + str(self.strides) + ' must be '\n",
        "                                     'greater than output padding ' +str(self.output_padding))\n",
        "                    \n",
        "        super(Conv2DAdaptive, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "        if input_shape[channel_axis] is None:\n",
        "            raise ValueError('The channel dimension of the inputs '\n",
        "                             'should be defined. Found `None`.')\n",
        "        input_dim = input_shape[channel_axis]\n",
        "        \n",
        "        self.input_channels = input_dim\n",
        "        kernel_shape = self.kernel_size + (input_dim, self.nfilters)\n",
        "        print(\"kernel shape:\",kernel_shape)\n",
        "\n",
        "        self.bias = None\n",
        "        # Set input spec.\n",
        "        self.input_spec = InputSpec(ndim=self.rank + 2,\n",
        "                                    axes={channel_axis: input_dim})\n",
        "        self.built = True\n",
        "        # Create a trainable weight variable for this layer.\n",
        "        \n",
        "        kernel_size = self.kernel_size\n",
        "        # Idxs Init\n",
        "        #mu = np.array([kernel_size[0] // 2, kernel_size[1] // 2])\n",
        "        mu = np.array([0.5, 0.5])\n",
        "\n",
        "\n",
        "        # Convert Types\n",
        "        self.mu = mu.astype(dtype='float32')\n",
        "\n",
        "        # Shared Parameters\n",
        "        # below works for only two dimensional cov \n",
        "        #self.cov = self.add_weight(shape=[input_dim*self.filters,2,2], \n",
        "        #                          name=\"cov\", initializer=cov_init, trainable=False)\n",
        "        \n",
        "        #from functools import partial\n",
        "\n",
        "        #sigma_initializer = partial(sigma_init,initsigma=self.initsigma)\n",
        "        \n",
        "\n",
        "\n",
        "    \n",
        "        self.idxs= idx_init(shape=[kernel_size[0]*kernel_size[1],2])\n",
        "        \n",
        "        self.W = self.add_weight(shape=[kernel_size[0],kernel_size[1],\n",
        "                                        self.input_channels,self.nfilters],\n",
        "                                 name='Weights',initializer=self.initW,\n",
        "                                 trainable=True,#self.trainWeights,\n",
        "                                 constraint=None)\n",
        "        \n",
        "        self.Sigma = self.add_weight(shape=(self.nfilters,),\n",
        "                                          name='Sigma',\n",
        "                                          initializer=initializers.Constant(self.initsigma),\n",
        "                                          trainable=self.trainsigmas,\n",
        "                                          constraint= constraints.NonNeg())        \n",
        "#        self.kernel = self.add_weight(shape=kernel_shape,\n",
        "#                                      initializer=initializers.,\n",
        "#                                      name='kernel',trainable=False,\n",
        "#                                      regularizer=None,\n",
        "#                                      constraint=None)\n",
        "        \n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(shape=(self.nfilters,),\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        name='bias',\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        \n",
        "        super(Conv2DAdaptive, self).build(input_shape)  # Be sure to call this somewhere!\n",
        "        \n",
        "\n",
        "    \n",
        "    def U(self):\n",
        "  \n",
        "        #e1 = (self.idxs - self.mu)\n",
        "        #print(\"e1.shape\",e1.shape)\n",
        "        #print(\"cov scaler shape\",self.cov_scaler)\n",
        "   \n",
        "        #print(self.cov.shape)\n",
        "        #print(len(tf.unstack(self.cov,axis=0)))\n",
        "        #print( tf.linalg.inv(tf.unstack(self.cov,axis=0)[0]))\n",
        "        # tensorflow does not need scan it does the same op to all covs.\n",
        "        #cov_inv = self.cov\n",
        "        #cov_scaled =self.cov_scaler*self.cov\n",
        "#        cov_scaled = tf.scalar_mul(self.cov_scaler,self.cov)\n",
        "#        print(self.cov.shape, self.cov_scaler.shape )\n",
        "#        cov_scaled = K.batch_dot(self.cov_scaler,self.cov, axes=[1,2])\n",
        "        #cov_inv = tf.linalg.inv(cov_scaled)\n",
        "        #print(\"cov_scaled :\",cov_scaled.shape)\n",
        "        #cov_inv = K.map_fn(lambda x: tf.linalg.inv(x), elems=tf.unstack(self.cov,axis=0))\n",
        "       \n",
        "\n",
        "        #e2 = K.dot(e1, K.transpose(cov_inv))\n",
        "        #ex = K.batch_dot(e2, e1, axes=[[1], [1]])\n",
        "        #result = K.exp(-(1 / 2.0) * ex)\n",
        "\n",
        "        up= K.sum((self.idxs - self.mu)**2, axis=1)\n",
        "        #print(\"up.shape\",up.shape)\n",
        "        up = K.expand_dims(up,axis=1,)\n",
        "        #print(\"up.shape\",up.shape)\n",
        "        # clipping scaler in range to prevent div by 0 or negative cov. \n",
        "        sigma = K.clip(self.Sigma,0.01,5.0)\n",
        "        #cov_scaler = self.cov_scaler\n",
        "        dwn = 2 * ( sigma ** 2)\n",
        "        #scaler = (np.pi*self.cov_scaler**2) * (self.idxs.shape[0])\n",
        "        result = K.exp(-up / dwn)\n",
        "        \n",
        "\n",
        "\n",
        "        # Transpose is super important.\n",
        "        #filter: A 4-D `Tensor` with the same type as `value` and shape\n",
        "        #`[height, width, in_channels,output_channels]`\n",
        "        # we do not care about input channels\n",
        "\n",
        "        masks = K.reshape(result,(self.kernel_size[0], \n",
        "                                  self.kernel_size[1],\n",
        "                                  1,self.nfilters))   \n",
        "\n",
        "         \n",
        "        #sum normalization each filter has sum 1\n",
        "        #sums = K.sum(masks**2, axis=(0, 1), keepdims=True)\n",
        "        #print(sums)\n",
        "        #gain = K.constant(self.gain, dtype='float32')\n",
        "        masks /= K.sqrt(K.sum(K.square(masks), axis=(0, 1),keepdims=True))\n",
        "        #masks /= (K.sum(masks, axis=(0, 1),keepdims=True)*self.input_channels)\n",
        "        #masks /= K.sum(masks, axis=(0, 1),keepdims=True)\n",
        "        #masks /= (self.kernel_size[0]*self.kernel_size[1])\n",
        "        \n",
        "        #masks *= (gain*np.sqrt(self.kernel_size[0]*self.kernel_size[1]))\n",
        "        #ums = sums * sums\n",
        "        #print(\"sums shape: \", sums.shape)\n",
        "        \n",
        "        # Sum normalisation\n",
        "        \n",
        "        #masks = masks * (gain/K.sqrt(sums))\n",
        "        #masks = masks * (gain/sums)\n",
        "        #print(\"masks shape\", masks.shape)\n",
        "        #print(\"masks mask\", K.mean(masks))\n",
        "        return masks\n",
        "\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = K.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        if self.data_format == 'channels_first':\n",
        "          h_axis, w_axis = 2, 3\n",
        "          c_axis= 1\n",
        "          \n",
        "        else:\n",
        "            h_axis, w_axis = 1, 2\n",
        "            c_axis=3\n",
        "            \n",
        "        ##BTEK \n",
        "        in_channels =input_shape[c_axis]\n",
        "        \n",
        "\n",
        "\n",
        "        ##BTEK \n",
        "        print(\"Calling self.U:\")\n",
        "        kernel = self.U()\n",
        "        print(\"kernel shape in output:\",kernel.shape)\n",
        "        if self.input_channels>1:\n",
        "            kernel = K.repeat_elements(kernel, self.input_channels, axis=2)\n",
        "            print(\"kernel reshaped :\",kernel.shape)\n",
        "        print(\"inputs shape\",inputs.shape)\n",
        "        #print(K.eval(kernel))\n",
        "        # multiply with weights\n",
        "        kernel = kernel*self.W\n",
        "        \n",
        "        #---------------------------------------------------------------------\n",
        "        print(\"Trainable weights\", self._trainable_weights)\n",
        "        outputs = K.conv2d(\n",
        "                inputs,\n",
        "                kernel,\n",
        "                strides=self.strides,\n",
        "                padding=self.padding,\n",
        "                data_format=self.data_format,\n",
        "                dilation_rate=self.dilation_rate)\n",
        "        \n",
        "        print(outputs.shape)\n",
        "\n",
        "        if self.use_bias:\n",
        "            outputs = K.bias_add(\n",
        "                outputs,\n",
        "                self.bias,\n",
        "                data_format=self.data_format)\n",
        "\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.data_format == 'channels_last':\n",
        "            space = input_shape[1:-1]\n",
        "        elif self.data_format == 'channels_first':\n",
        "            space = input_shape[2:]\n",
        "        new_space = []\n",
        "        for i in range(len(space)):\n",
        "            new_dim = conv_utils.conv_output_length(\n",
        "                space[i],\n",
        "                self.kernel_size[i],\n",
        "                padding=self.padding,\n",
        "                stride=self.strides[i],\n",
        "                dilation=self.dilation_rate[i])\n",
        "            new_space.append(new_dim)\n",
        "        if self.data_format == 'channels_last':\n",
        "            return (input_shape[0],) + tuple(new_space) + (self.nfilters,)\n",
        "        elif self.data_format == 'channels_first':\n",
        "            return (input_shape[0], self.filters) + tuple(new_space)\n",
        "        #return tuple(output_shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "#test\n",
        "def test():\n",
        "    import tensorflow as tf\n",
        "    #from gausslayer import GaussScaler\n",
        "    import numpy as np\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    from keras.losses import mse\n",
        "    import keras\n",
        "    from keras.datasets import mnist,fashion_mnist, cifar10\n",
        "    from keras.models import Sequential, Model\n",
        "    from keras.layers import Input, Dense, Dropout, Flatten\n",
        "    from skimage import filters\n",
        "    \n",
        "    \n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    inputimg = x_train[0]/255\n",
        "    sh = (inputimg.shape[0],inputimg.shape[1],1)\n",
        "    outputimages = np.zeros(shape=[inputimg.shape[0],inputimg.shape[1],3],dtype='float32')\n",
        "    outputimages[:,:,0] = filters.gaussian(inputimg,sigma=1)\n",
        "    outputimages[:,:,1] = filters.sobel_h(inputimg)\n",
        "    outputimages[:,:,2] = filters.sobel_v(filters.gaussian(inputimg,sigma=0.5))\n",
        "    \n",
        "    y = y_train[0]\n",
        "    \n",
        "    node_in = Input(shape=sh, name='inputlayer')\n",
        "    node_acnn = Conv2DAdaptive(rank=2,nfilters=3,kernel_size=(7,7), \n",
        "                             data_format='channels_last',\n",
        "                             padding='same',name='acnn',activation='tanh',\n",
        "                             init_sigma=0.1, trainsigmas=True, \n",
        "                             trainWeights=True)(node_in)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    model = Model(inputs=node_in, outputs=[node_acnn])\n",
        "    model.reset_states()\n",
        "   # model.summary()\n",
        "\n",
        "    \n",
        "    sgd = keras.optimizers.SGD(lr=3.0, decay=0.000,momentum=0.0, nesterov=False)\n",
        "    model.compile(loss=mse, optimizer=sgd, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    \n",
        "    inputimg2 = np.expand_dims(np.expand_dims(inputimg,axis=0), axis=3)\n",
        "    outputimages2 = np.expand_dims(outputimages,axis=0)\n",
        "    \n",
        "    model.fit(inputimg2, outputimages2,\n",
        "              batch_size=1,\n",
        "              epochs=50,\n",
        "              verbose=1)\n",
        "\n",
        "    \n",
        "    acnn_layer = model.get_layer('acnn')    \n",
        "    all_params=acnn_layer.weights\n",
        "    print(\"All params:\",all_params)\n",
        "    acnn_params = acnn_layer.get_weights()\n",
        "    for i,v in enumerate(all_params):\n",
        "        print(v, \":\", acnn_params[i],\"\\n\")\n",
        "    #out = acnn_layer.get_output_at(0)\n",
        "#\n",
        "#    with tf.Session() as sess:\n",
        "#        sess.run(tf.global_variables_initializer())\n",
        "#        outval = sess.run(out, feed_dict={inputs:inputimg2})\n",
        "#        acnn_layer_var = acnn_layer.get_weights()\n",
        "    \n",
        "    \n",
        "    pred_images = model.predict(inputimg2,  verbose=1)\n",
        "    print(\"Prediction shape\",pred_images.shape)\n",
        "    plt = True\n",
        "    if plt:\n",
        "        print(\"Plotting kernels before...\")\n",
        "        import matplotlib.pyplot as plt\n",
        "        num_images=min(pred_images.shape[3],12)\n",
        "        fig=plt.figure(figsize=(10,5))\n",
        "        plt.subplot(3, num_images, 2)\n",
        "        plt.imshow(np.squeeze(inputimg2[0,:,:,0]))\n",
        "        for i in range(num_images):\n",
        "            plt.subplot(3, num_images, i+4)\n",
        "            plt.imshow(np.squeeze(outputimages2[0,:,:,i]))\n",
        "            print(\"Max-in:\",i,\" \",np.max(np.squeeze(outputimages2[0,:,:,i])))\n",
        "        \n",
        "        for i in range(num_images):\n",
        "            plt.subplot(3, num_images, i+7)\n",
        "            plt.imshow(np.squeeze(pred_images[0,:,:,i]))\n",
        "            print(\"MAx:\",\"pred\",i,np.max(np.squeeze(pred_images[0,:,:,i])))\n",
        "        #fig.colorbar(im, ax=ax1)\n",
        "        plt.show()\n",
        "        \n",
        "        \n",
        "    print( model.get_layer('acnn').output )"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atu9imf6gQhl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8983
        },
        "outputId": "076229b9-a4d4-4bae-e590-2e9358e3de60"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Fri Jun 21 17:27:34 2019\n",
        "\n",
        "@author: btek\n",
        "\"\"\"\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist,cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import BatchNormalization,Activation\n",
        "from keras.optimizers import SGD\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "#dset='cifar10'#mnist\n",
        "dset='mnist'\n",
        "batch_size = 512\n",
        "num_classes = 10\n",
        "epochs = 500\n",
        "\n",
        "if dset=='mnist':\n",
        "    # input image dimensions\n",
        "    img_rows, img_cols = 28, 28  \n",
        "    # the data, split between train and test sets\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    n_channels=1\n",
        "\n",
        "elif dset=='cifar10':    \n",
        "    img_rows, img_cols = 32,32\n",
        "    n_channels=3\n",
        "    \n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], n_channels, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], n_channels, img_rows, img_cols)\n",
        "    input_shape = (n_channels, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, n_channels)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, n_channels)\n",
        "    input_shape = (img_rows, img_cols, n_channels)\n",
        "        \n",
        "\n",
        "    \n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                   activation='linear',padding='same',\n",
        "          input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(32, (3, 3), activation='linear',padding='same'))\n",
        "#odel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "#odel.add(Dense(128, activation='relu'))\n",
        "\n",
        "#odel.add(Dropout(0.25))\n",
        "\n",
        "  #=============================================================================\n",
        "model.add(Conv2DAdaptive(rank=2,nfilters=32,kernel_size=(5,5), \n",
        "                       data_format='channels_last',strides=1,\n",
        "                       padding='same',name='acnn-1', activation='linear'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2DAdaptive(rank=2,nfilters=32,kernel_size=(5,5), \n",
        "                       data_format='channels_last',strides=1,\n",
        "                       padding='same',name='acnn-2', activation='linear'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "  \n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "   \n",
        "#=============================================================================\n",
        "    \n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "from lr_multiplier import LearningRateMultiplier\n",
        "\n",
        "multipliers = {'acnn-1': 1.0,'acnn-2': 1.0}\n",
        "opt = LearningRateMultiplier(SGD, lr_multipliers=multipliers, \n",
        "                             lr=0.01, momentum=0.0,decay=0.00)\n",
        "print(opt)\n",
        "#opt = SGD(lr=0.01,momentum=0.5)\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "plt = True\n",
        "if plt:\n",
        "    print(\"Plotting kernels before...\")\n",
        "    import matplotlib.pyplot as plt\n",
        "    acnn_layer = model.get_layer('acnn-1')\n",
        "    ws = acnn_layer.get_weights()\n",
        "    print(\"Sigmas before\",ws[0])\n",
        "    u_func = K.function(inputs=[model.input], outputs=[acnn_layer.U()])\n",
        "    output_func = K.function(inputs=[model.input], outputs=[acnn_layer.output])\n",
        "\n",
        "    U_val=u_func([np.expand_dims(x_test[0], axis=0)])\n",
        "    \n",
        "    print(\"U shape\", U_val[0].shape)\n",
        "    print(\"U max:\", np.max(U_val[0][:,:,:,:]))\n",
        "    num_filt=min(U_val[0].shape[3],12)\n",
        "    fig=plt.figure(figsize=(10,5))\n",
        "    for i in range(num_filt):\n",
        "        ax1=plt.subplot(1, num_filt, i+1)\n",
        "        im = ax1.imshow(np.squeeze(U_val[0][:,:,0,i]))\n",
        "    fig.colorbar(im, ax=ax1)\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "\n",
        "if plt:\n",
        "    print(\"Plotting kernels after ...\")\n",
        "    \n",
        "    print(\"U max:\", np.max(U_val[0][:,:,:,:]))\n",
        "    import matplotlib.pyplot as plt\n",
        "    ws = acnn_layer.get_weights()\n",
        "    print(\"Sigmas after\",ws[0])\n",
        "    U_val=u_func([np.expand_dims(x_test[2], axis=0)])\n",
        "    \n",
        "    print(\"U shape\", U_val[0].shape)\n",
        "    num_filt=min(U_val[0].shape[3],12)\n",
        "    fig=plt.figure(figsize=(16,5))\n",
        "    for i in range(num_filt):\n",
        "        ax=plt.subplot(1, num_filt, i+1)\n",
        "        im = ax.imshow(np.squeeze(U_val[0][:,:,0,i]))\n",
        "    #fig.colorbar(im, ax=ax1)\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "    print(\"outputs  ...\")\n",
        "    \n",
        "    n=5\n",
        "    \n",
        "    out_val=output_func([np.expand_dims(x_test[5], axis=0)])\n",
        "    print(\"Outputs shape\", out_val[0].shape)\n",
        "    num_filt=min(out_val[0].shape[3],12)\n",
        "    fig=plt.figure(figsize=(16,10))\n",
        "    ax=plt.subplot(1, num_filt+1, 1)\n",
        "    im = ax.imshow(np.squeeze(x_test[5]))\n",
        "    print(y_test[5])\n",
        "    print(\"input mean,var,max\",np.mean(x_test[n]),np.var(x_test[n]),np.max(x_test[n]))\n",
        "    for i in range(num_filt):\n",
        "        ax=plt.subplot(1, num_filt+1, i+2)\n",
        "        out_im = out_val[0][0,:,:,i]\n",
        "        im = ax.imshow(np.squeeze(out_im))\n",
        "        print(\"ouput mean,var,max\",np.mean(out_im),\n",
        "                                       np.var(out_im),\n",
        "                                       np.max(out_im))\n",
        "        #plt.colorbar(im,ax=ax)\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "{'name': 'acnn-1'}\n",
            "kernel shape: (5, 5, 32, 32)\n",
            "Calling self.U:\n",
            "kernel shape in output: (5, 5, 1, 32)\n",
            "kernel reshaped : (5, 5, 32, 32)\n",
            "inputs shape (?, 28, 28, 32)\n",
            "Trainable weights [<tf.Variable 'acnn-1_3/Weights:0' shape=(5, 5, 32, 32) dtype=float32_ref>, <tf.Variable 'acnn-1_3/Sigma:0' shape=(32,) dtype=float32_ref>]\n",
            "(?, 28, 28, 32)\n",
            "{'name': 'acnn-2'}\n",
            "kernel shape: (5, 5, 32, 32)\n",
            "Calling self.U:\n",
            "kernel shape in output: (5, 5, 1, 32)\n",
            "kernel reshaped : (5, 5, 32, 32)\n",
            "inputs shape (?, 14, 14, 32)\n",
            "Trainable weights [<tf.Variable 'acnn-2_3/Weights:0' shape=(5, 5, 32, 32) dtype=float32_ref>, <tf.Variable 'acnn-2_3/Sigma:0' shape=(32,) dtype=float32_ref>]\n",
            "(?, 14, 14, 32)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_14 (Conv2D)           (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "acnn-1 (Conv2DAdaptive)      (None, 28, 28, 32)        25632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "acnn-2 (Conv2DAdaptive)      (None, 14, 14, 32)        25632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 14, 14, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 1568)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 128)               200832    \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 263,466\n",
            "Trainable params: 263,210\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n",
            "<lr_multiplier.LearningRateMultiplier object at 0x7f50504b7390>\n",
            "Plotting kernels before...\n",
            "Sigmas before [[[[ 0.01870632  0.02431283  0.05148296 ...  0.00561346  0.04113408\n",
            "    -0.0249009 ]\n",
            "   [-0.04428443 -0.04047427  0.01935207 ... -0.00884125 -0.05318487\n",
            "    -0.00762497]\n",
            "   [-0.02851091 -0.04912481  0.02674234 ...  0.04380013 -0.01951926\n",
            "    -0.03613108]\n",
            "   ...\n",
            "   [ 0.04603813 -0.04925577  0.00401763 ... -0.03190158  0.00130636\n",
            "     0.00632102]\n",
            "   [ 0.02827032 -0.02870617 -0.03092587 ...  0.0520567   0.01853241\n",
            "    -0.04506633]\n",
            "   [-0.04515703 -0.03943803  0.00860984 ...  0.05380733  0.01094243\n",
            "     0.00963459]]\n",
            "\n",
            "  [[ 0.00970494  0.05919555 -0.05007033 ... -0.00610051 -0.05616536\n",
            "    -0.05563177]\n",
            "   [-0.04962857  0.05844284  0.01447455 ... -0.02388103 -0.03418173\n",
            "    -0.0595852 ]\n",
            "   [ 0.01907929  0.00495121  0.02117103 ... -0.00602986  0.047428\n",
            "    -0.05564948]\n",
            "   ...\n",
            "   [-0.02108818 -0.01404471 -0.01158071 ...  0.02216254 -0.03183878\n",
            "    -0.05883459]\n",
            "   [-0.03378005  0.05814977  0.00364486 ... -0.06052604  0.00845251\n",
            "    -0.06004858]\n",
            "   [ 0.01312937 -0.01845576  0.01228163 ...  0.05126733  0.01537381\n",
            "    -0.01471586]]\n",
            "\n",
            "  [[-0.04951732  0.05261542 -0.02228595 ... -0.05760185 -0.01602815\n",
            "    -0.02470148]\n",
            "   [-0.05247746 -0.04033639 -0.03098911 ...  0.02610548  0.03255646\n",
            "     0.04826316]\n",
            "   [ 0.03949662 -0.01310061 -0.00288315 ... -0.00430656 -0.05047982\n",
            "     0.02397199]\n",
            "   ...\n",
            "   [ 0.05108181 -0.00611629  0.01868196 ... -0.02836917  0.05688678\n",
            "    -0.05708099]\n",
            "   [ 0.01069914 -0.06029787  0.01963979 ...  0.04226009 -0.00031767\n",
            "    -0.04576993]\n",
            "   [ 0.0271961  -0.00768236 -0.05565035 ... -0.00034744  0.03369213\n",
            "     0.00908786]]\n",
            "\n",
            "  [[ 0.01461369  0.0283765   0.04119189 ...  0.05261339 -0.01510843\n",
            "    -0.00271587]\n",
            "   [ 0.04119949  0.05320272 -0.04217546 ...  0.01203795  0.01943776\n",
            "     0.01862324]\n",
            "   [-0.03568833  0.01006738 -0.01538656 ...  0.04406487  0.03280633\n",
            "     0.03041393]\n",
            "   ...\n",
            "   [-0.01234737  0.04677263  0.05139175 ...  0.04098118 -0.04500478\n",
            "    -0.01223892]\n",
            "   [ 0.02057451 -0.04471033  0.01091314 ...  0.05778803  0.00589469\n",
            "    -0.0490533 ]\n",
            "   [-0.02233701 -0.02714689 -0.03116233 ...  0.03280405  0.02515586\n",
            "     0.04886918]]\n",
            "\n",
            "  [[-0.05668692  0.02254912  0.04082884 ... -0.05307496 -0.01681716\n",
            "    -0.03719452]\n",
            "   [ 0.04434023  0.04181207  0.03644206 ... -0.0319506  -0.04184137\n",
            "    -0.05821707]\n",
            "   [-0.00725305 -0.027079    0.01292798 ...  0.01334893 -0.01749134\n",
            "     0.01614311]\n",
            "   ...\n",
            "   [-0.00849267 -0.03273883 -0.05790919 ...  0.01610162  0.05672492\n",
            "    -0.0433965 ]\n",
            "   [-0.04619746  0.05038168 -0.05995262 ...  0.00533435 -0.03809497\n",
            "     0.0083089 ]\n",
            "   [-0.03077689  0.01896868 -0.0451919  ...  0.04926648  0.01535277\n",
            "    -0.01785285]]]\n",
            "\n",
            "\n",
            " [[[ 0.00637572 -0.01258969  0.0094762  ...  0.05135733 -0.05907091\n",
            "     0.03675796]\n",
            "   [ 0.05531563  0.02803921  0.04446558 ...  0.02648024 -0.06059773\n",
            "     0.00069842]\n",
            "   [ 0.03563432  0.02165783  0.02740859 ... -0.01052886  0.0454718\n",
            "    -0.00437847]\n",
            "   ...\n",
            "   [-0.0540543   0.02766174  0.01263041 ... -0.01918558  0.03996413\n",
            "    -0.05529214]\n",
            "   [ 0.05129762 -0.01631366  0.0046983  ... -0.04561213  0.03326761\n",
            "    -0.03863329]\n",
            "   [ 0.04448747 -0.00380271  0.05547101 ... -0.04627362 -0.02448837\n",
            "     0.03504327]]\n",
            "\n",
            "  [[ 0.04497704 -0.02212382 -0.06106534 ... -0.03108648  0.03507024\n",
            "     0.03156811]\n",
            "   [-0.02742259 -0.01926269  0.0597853  ...  0.03553921 -0.00597001\n",
            "    -0.04572727]\n",
            "   [-0.03007185  0.02469127  0.05885643 ...  0.05916839 -0.04950126\n",
            "     0.01501724]\n",
            "   ...\n",
            "   [-0.00472297 -0.02076472 -0.03037239 ... -0.00014123 -0.01421748\n",
            "    -0.02995129]\n",
            "   [-0.01664398 -0.05643796 -0.00605989 ... -0.05956359 -0.03392664\n",
            "    -0.0097738 ]\n",
            "   [ 0.05763163 -0.0207193   0.00997668 ...  0.00249248 -0.05743835\n",
            "    -0.04004654]]\n",
            "\n",
            "  [[-0.03052768 -0.04988968  0.01712511 ...  0.00991958  0.0464316\n",
            "    -0.03417266]\n",
            "   [ 0.05719608 -0.01111409 -0.02653882 ... -0.03806999  0.01009992\n",
            "    -0.00265292]\n",
            "   [-0.01797608 -0.04907024  0.04172458 ... -0.05434351  0.00046301\n",
            "     0.00947909]\n",
            "   ...\n",
            "   [ 0.06012619  0.01921024  0.04155678 ...  0.02960097  0.00643885\n",
            "     0.03155739]\n",
            "   [-0.03758696  0.03797976 -0.05143119 ... -0.05083986 -0.02113606\n",
            "    -0.01323071]\n",
            "   [-0.03769837  0.04560522 -0.00860448 ...  0.03484313  0.01699923\n",
            "    -0.04242315]]\n",
            "\n",
            "  [[-0.01732701  0.01513969  0.04750531 ...  0.0413049   0.02912112\n",
            "     0.01372432]\n",
            "   [-0.05115669  0.04045289 -0.06098888 ... -0.05092411  0.00513468\n",
            "    -0.0175783 ]\n",
            "   [-0.00728466 -0.02402589  0.02610395 ... -0.06010213 -0.01436323\n",
            "    -0.0239204 ]\n",
            "   ...\n",
            "   [-0.0444012  -0.03461146 -0.04663475 ...  0.01382822  0.03061481\n",
            "    -0.00633883]\n",
            "   [ 0.02790571  0.01809504 -0.06109993 ... -0.00052978  0.00376215\n",
            "    -0.02918476]\n",
            "   [ 0.0325272   0.01316828 -0.04772799 ... -0.0065018  -0.01776931\n",
            "     0.05665103]]\n",
            "\n",
            "  [[-0.02271724  0.0151203   0.04978311 ...  0.05286794  0.05735664\n",
            "     0.01397255]\n",
            "   [-0.04946707 -0.03835565 -0.00170523 ... -0.01174696  0.04740353\n",
            "    -0.00376996]\n",
            "   [-0.04939112 -0.03794847 -0.02058433 ... -0.03278132 -0.04250583\n",
            "    -0.05191905]\n",
            "   ...\n",
            "   [-0.02428062  0.02941646 -0.03133385 ... -0.00751509 -0.01668448\n",
            "    -0.02135692]\n",
            "   [-0.03381873  0.027914   -0.05755986 ...  0.01053485  0.02971761\n",
            "     0.03444865]\n",
            "   [-0.01935785  0.01613292 -0.04457428 ... -0.03998674 -0.03314687\n",
            "     0.01090151]]]\n",
            "\n",
            "\n",
            " [[[-0.0429551  -0.01127012  0.05197592 ...  0.05450786 -0.02607746\n",
            "    -0.01882174]\n",
            "   [ 0.02393675  0.05015766  0.01346596 ...  0.03917577 -0.03562078\n",
            "    -0.03235736]\n",
            "   [-0.04632903 -0.03445327 -0.04527309 ... -0.03595116  0.03166869\n",
            "    -0.01872091]\n",
            "   ...\n",
            "   [ 0.060855   -0.02575298  0.06050976 ... -0.05290421  0.05471484\n",
            "    -0.03695612]\n",
            "   [-0.00978056  0.01811482 -0.03303548 ...  0.01874965 -0.03762813\n",
            "    -0.02307368]\n",
            "   [-0.02230281  0.00944264  0.02514709 ... -0.03138972  0.05009389\n",
            "     0.01274932]]\n",
            "\n",
            "  [[-0.05505334  0.05341016 -0.04295201 ... -0.0412955  -0.02882948\n",
            "     0.02248241]\n",
            "   [-0.03953052  0.03919302 -0.0199149  ...  0.01900759 -0.03927802\n",
            "    -0.00562325]\n",
            "   [ 0.01382602 -0.03769429  0.00707705 ... -0.04690515  0.0484011\n",
            "    -0.00098281]\n",
            "   ...\n",
            "   [-0.01775734  0.01785697  0.02989173 ... -0.0408421  -0.04087234\n",
            "    -0.0135481 ]\n",
            "   [-0.03277949 -0.05346837 -0.03831292 ...  0.02864365 -0.00581817\n",
            "     0.0483679 ]\n",
            "   [ 0.05904561 -0.02409305  0.04140282 ... -0.04147657 -0.04171287\n",
            "     0.05533923]]\n",
            "\n",
            "  [[-0.06059114 -0.02080214  0.00763737 ... -0.0578191   0.04414038\n",
            "     0.00445023]\n",
            "   [-0.0383149  -0.03370479  0.01281573 ... -0.055583   -0.01423625\n",
            "     0.03971234]\n",
            "   [ 0.01135941 -0.03277441 -0.00587343 ... -0.00261508  0.01154425\n",
            "     0.01615327]\n",
            "   ...\n",
            "   [-0.02413263 -0.04458427  0.03487071 ...  0.02849669  0.00130321\n",
            "    -0.04304387]\n",
            "   [-0.00951296  0.04756356  0.01571026 ... -0.00923999  0.01766777\n",
            "    -0.00269515]\n",
            "   [-0.051341    0.01500878 -0.01139647 ... -0.00361779  0.05615974\n",
            "    -0.03300367]]\n",
            "\n",
            "  [[ 0.0101464  -0.0292628  -0.00305739 ...  0.00548075  0.00733126\n",
            "     0.01118415]\n",
            "   [ 0.05160461 -0.05189073 -0.02314338 ...  0.05354016 -0.00796815\n",
            "     0.05345966]\n",
            "   [ 0.05106169  0.0244636   0.00126033 ... -0.04191035  0.0497709\n",
            "    -0.04096346]\n",
            "   ...\n",
            "   [-0.00628165 -0.04930438  0.0138634  ...  0.03957632  0.04606904\n",
            "    -0.04697046]\n",
            "   [-0.00760471  0.01297973  0.05481888 ...  0.01691811 -0.04084453\n",
            "    -0.0575989 ]\n",
            "   [ 0.06069987  0.02680566 -0.03794914 ... -0.02503584 -0.03512799\n",
            "    -0.01484906]]\n",
            "\n",
            "  [[-0.04836718  0.02989986 -0.03419141 ... -0.03905819 -0.05121038\n",
            "    -0.00272569]\n",
            "   [-0.04376617 -0.04952345  0.05672517 ...  0.05895298  0.0544987\n",
            "     0.00823246]\n",
            "   [-0.02458681 -0.04640229  0.03486146 ... -0.00975035 -0.0328019\n",
            "    -0.01671315]\n",
            "   ...\n",
            "   [ 0.0412923   0.04099249  0.05994778 ... -0.01668052  0.02742301\n",
            "    -0.04626328]\n",
            "   [ 0.00852927  0.00279066  0.02272024 ...  0.00121303 -0.00718037\n",
            "     0.05907637]\n",
            "   [-0.01126203  0.01139555  0.03588629 ... -0.05968008  0.03249682\n",
            "     0.01787887]]]\n",
            "\n",
            "\n",
            " [[[-0.03405359  0.01670115 -0.00733205 ... -0.02467539  0.01683683\n",
            "    -0.00861855]\n",
            "   [ 0.03093379 -0.00561126 -0.00304084 ... -0.0506242  -0.00343688\n",
            "    -0.00229744]\n",
            "   [-0.04342245 -0.04102249  0.00219415 ... -0.03710315 -0.00024328\n",
            "    -0.02456395]\n",
            "   ...\n",
            "   [ 0.00040051  0.03633111 -0.01977431 ...  0.01777118 -0.01531112\n",
            "     0.01861331]\n",
            "   [-0.00216755  0.00092654  0.01050435 ...  0.04424788  0.03038662\n",
            "    -0.06033352]\n",
            "   [ 0.02241894  0.01609298  0.00011788 ...  0.05074518 -0.02502983\n",
            "     0.0244677 ]]\n",
            "\n",
            "  [[ 0.0068334   0.03888595  0.01042527 ... -0.02444213 -0.0425109\n",
            "     0.05085288]\n",
            "   [-0.02113685  0.04519921 -0.01122236 ...  0.05988719  0.0016878\n",
            "    -0.00020233]\n",
            "   [-0.01955749 -0.02494625  0.02276867 ...  0.04001776  0.04410168\n",
            "    -0.00589223]\n",
            "   ...\n",
            "   [-0.03568709 -0.02254709  0.05465711 ... -0.0173361   0.00289482\n",
            "    -0.03344135]\n",
            "   [-0.052738    0.02603279  0.00378328 ...  0.00829187  0.00535771\n",
            "    -0.0276847 ]\n",
            "   [-0.02131826 -0.01011486 -0.04630518 ... -0.0567835   0.02694037\n",
            "    -0.00523466]]\n",
            "\n",
            "  [[-0.04796726 -0.01220064  0.00347768 ...  0.01230109  0.03316182\n",
            "    -0.04175957]\n",
            "   [ 0.01410736 -0.03282794 -0.04037851 ... -0.03527239  0.03000217\n",
            "     0.04031641]\n",
            "   [-0.04662441  0.00907006 -0.04694717 ...  0.03854674 -0.010806\n",
            "    -0.0114503 ]\n",
            "   ...\n",
            "   [ 0.03728708 -0.05545175 -0.04416516 ... -0.05163509 -0.05973377\n",
            "     0.02602295]\n",
            "   [-0.04074334 -0.01897658 -0.00082762 ...  0.00224275  0.01456129\n",
            "    -0.04381689]\n",
            "   [-0.05419438  0.04258599 -0.04134144 ... -0.03871427 -0.01503667\n",
            "    -0.05011763]]\n",
            "\n",
            "  [[-0.05035832  0.05132928 -0.0507483  ...  0.05674139 -0.01235428\n",
            "     0.04048737]\n",
            "   [ 0.0498737  -0.02882184 -0.05100465 ... -0.04355937 -0.01858389\n",
            "    -0.0043662 ]\n",
            "   [-0.05054111  0.02340263 -0.03693692 ... -0.03979132 -0.02398822\n",
            "    -0.03406004]\n",
            "   ...\n",
            "   [-0.01502186  0.0359051  -0.00820093 ... -0.01616622  0.06111625\n",
            "    -0.03308834]\n",
            "   [-0.01943463  0.037731   -0.01383758 ... -0.05306717  0.03928728\n",
            "    -0.04030725]\n",
            "   [ 0.0523213  -0.00369223 -0.03710941 ...  0.02339429  0.05471434\n",
            "    -0.04346066]]\n",
            "\n",
            "  [[-0.03689214 -0.01871655 -0.05651783 ... -0.02092318 -0.01111447\n",
            "    -0.04312103]\n",
            "   [-0.00384418 -0.00729027 -0.04659542 ... -0.01035886 -0.01519772\n",
            "     0.05281718]\n",
            "   [-0.05368985  0.04840374 -0.03127319 ...  0.04470368 -0.02628973\n",
            "     0.00136885]\n",
            "   ...\n",
            "   [ 0.02157412  0.05708287  0.01570914 ...  0.00260396 -0.01247291\n",
            "     0.03390485]\n",
            "   [-0.03013499  0.05319792  0.02865148 ...  0.04508859 -0.05396105\n",
            "     0.02286463]\n",
            "   [-0.03213487  0.00421359  0.0110251  ...  0.01163312  0.00235673\n",
            "    -0.01610931]]]\n",
            "\n",
            "\n",
            " [[[-0.01070213  0.00783728  0.05752917 ... -0.00967039  0.030715\n",
            "    -0.01904415]\n",
            "   [-0.00334701  0.05107842  0.03390017 ... -0.05884897 -0.04334375\n",
            "    -0.03972927]\n",
            "   [ 0.05961048  0.05894877  0.05720663 ... -0.01995132  0.01531487\n",
            "    -0.00028007]\n",
            "   ...\n",
            "   [-0.04570156  0.03517509  0.00322252 ...  0.05761627  0.01969909\n",
            "    -0.04943241]\n",
            "   [ 0.02316511  0.01196554  0.02722828 ...  0.02195607  0.00477898\n",
            "    -0.0490532 ]\n",
            "   [ 0.0060367  -0.03224262  0.06048131 ...  0.0464528  -0.02249686\n",
            "    -0.04369269]]\n",
            "\n",
            "  [[ 0.00060742  0.04666617 -0.05833    ...  0.05284198 -0.02743218\n",
            "     0.00268831]\n",
            "   [-0.03859065 -0.03349307 -0.01013558 ...  0.05849696  0.04010905\n",
            "    -0.00753813]\n",
            "   [ 0.02248542  0.00832048  0.00226909 ...  0.03699501  0.00512923\n",
            "    -0.04496425]\n",
            "   ...\n",
            "   [ 0.05868754  0.05416434  0.04606871 ... -0.04925744  0.04434903\n",
            "    -0.00829658]\n",
            "   [ 0.00443188 -0.03495786 -0.0160763  ... -0.04673798  0.00617161\n",
            "    -0.00137285]\n",
            "   [-0.02856265  0.04345094 -0.01738524 ... -0.05290171  0.03958996\n",
            "     0.03694251]]\n",
            "\n",
            "  [[-0.03248365 -0.04951064  0.05760622 ...  0.06023953  0.01636211\n",
            "     0.00503054]\n",
            "   [-0.03163753  0.04907114  0.04084624 ...  0.04595865 -0.05105226\n",
            "    -0.02736692]\n",
            "   [-0.03817832 -0.00145554  0.00744132 ... -0.05327829 -0.03986875\n",
            "     0.00182455]\n",
            "   ...\n",
            "   [-0.02273197  0.04263123  0.0498877  ...  0.05769669  0.05166241\n",
            "     0.0147529 ]\n",
            "   [-0.01710394 -0.04758918 -0.05105618 ...  0.04613161  0.05934139\n",
            "     0.05796747]\n",
            "   [-0.02695403 -0.04370034 -0.01240467 ... -0.06053758  0.00632416\n",
            "    -0.05949463]]\n",
            "\n",
            "  [[-0.05518089  0.04454386  0.02267596 ... -0.03095244 -0.02794883\n",
            "     0.019961  ]\n",
            "   [-0.00553773  0.00166329  0.03934511 ...  0.0457701   0.05016814\n",
            "    -0.03435959]\n",
            "   [ 0.02247813 -0.05442984  0.05545328 ... -0.00150822 -0.01807377\n",
            "    -0.05795715]\n",
            "   ...\n",
            "   [ 0.05076388  0.01629193 -0.00221435 ... -0.02013737  0.02539478\n",
            "    -0.00569215]\n",
            "   [-0.0220326  -0.02289429  0.04235656 ...  0.00479292 -0.02557543\n",
            "     0.00651474]\n",
            "   [ 0.05923792  0.04015018 -0.05701098 ...  0.00682526  0.01976788\n",
            "    -0.03279824]]\n",
            "\n",
            "  [[-0.04327131 -0.0477729  -0.00648932 ... -0.03275413 -0.03584382\n",
            "    -0.04825176]\n",
            "   [-0.01769704  0.061037   -0.00819699 ... -0.05729694 -0.00195262\n",
            "     0.04029212]\n",
            "   [-0.0308904  -0.03195421  0.01046469 ... -0.0255159  -0.03080937\n",
            "     0.05801747]\n",
            "   ...\n",
            "   [-0.00017064 -0.02636245  0.01833085 ...  0.04079937 -0.01093024\n",
            "     0.00129526]\n",
            "   [-0.04151481  0.0163114  -0.02970811 ...  0.04767682  0.0035726\n",
            "     0.05136073]\n",
            "   [ 0.04625401  0.04333677 -0.06073998 ...  0.03379036 -0.05465488\n",
            "    -0.02063064]]]]\n",
            "U shape (5, 5, 1, 32)\n",
            "U max: 0.99615395\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAACNCAYAAAAHDFZsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJTgEAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x360 with 13 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "W0622 10:08:20.710179 139984547719040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/500\n",
            "60000/60000 [==============================] - 18s 305us/step - loss: 0.9462 - acc: 0.7034 - val_loss: 0.3355 - val_acc: 0.9248\n",
            "Epoch 2/500\n",
            "60000/60000 [==============================] - 13s 217us/step - loss: 0.3821 - acc: 0.8855 - val_loss: 0.2113 - val_acc: 0.9484\n",
            "Epoch 3/500\n",
            "60000/60000 [==============================] - 13s 215us/step - loss: 0.2704 - acc: 0.9199 - val_loss: 0.1721 - val_acc: 0.9556\n",
            "Epoch 4/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.2210 - acc: 0.9347 - val_loss: 0.1476 - val_acc: 0.9615\n",
            "Epoch 5/500\n",
            "60000/60000 [==============================] - 13s 213us/step - loss: 0.1918 - acc: 0.9420 - val_loss: 0.1316 - val_acc: 0.9647\n",
            "Epoch 6/500\n",
            "60000/60000 [==============================] - 13s 213us/step - loss: 0.1745 - acc: 0.9475 - val_loss: 0.1233 - val_acc: 0.9672\n",
            "Epoch 7/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.1568 - acc: 0.9523 - val_loss: 0.1055 - val_acc: 0.9720\n",
            "Epoch 8/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.1449 - acc: 0.9563 - val_loss: 0.0996 - val_acc: 0.9729\n",
            "Epoch 9/500\n",
            "60000/60000 [==============================] - 13s 215us/step - loss: 0.1367 - acc: 0.9589 - val_loss: 0.0963 - val_acc: 0.9729\n",
            "Epoch 10/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.1288 - acc: 0.9608 - val_loss: 0.0933 - val_acc: 0.9739\n",
            "Epoch 11/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.1227 - acc: 0.9626 - val_loss: 0.0933 - val_acc: 0.9749\n",
            "Epoch 12/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.1167 - acc: 0.9642 - val_loss: 0.0936 - val_acc: 0.9736\n",
            "Epoch 13/500\n",
            "60000/60000 [==============================] - 13s 213us/step - loss: 0.1130 - acc: 0.9655 - val_loss: 0.0831 - val_acc: 0.9761\n",
            "Epoch 14/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.1075 - acc: 0.9673 - val_loss: 0.0837 - val_acc: 0.9762\n",
            "Epoch 15/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.1044 - acc: 0.9682 - val_loss: 0.0830 - val_acc: 0.9762\n",
            "Epoch 16/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.1021 - acc: 0.9692 - val_loss: 0.0761 - val_acc: 0.9786\n",
            "Epoch 17/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0980 - acc: 0.9712 - val_loss: 0.0738 - val_acc: 0.9799\n",
            "Epoch 18/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0952 - acc: 0.9717 - val_loss: 0.0767 - val_acc: 0.9779\n",
            "Epoch 19/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0911 - acc: 0.9725 - val_loss: 0.0707 - val_acc: 0.9793\n",
            "Epoch 20/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0906 - acc: 0.9724 - val_loss: 0.0677 - val_acc: 0.9801\n",
            "Epoch 21/500\n",
            "60000/60000 [==============================] - 13s 215us/step - loss: 0.0885 - acc: 0.9727 - val_loss: 0.0689 - val_acc: 0.9798\n",
            "Epoch 22/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0846 - acc: 0.9746 - val_loss: 0.0676 - val_acc: 0.9800\n",
            "Epoch 23/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0840 - acc: 0.9738 - val_loss: 0.0613 - val_acc: 0.9826\n",
            "Epoch 24/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0832 - acc: 0.9742 - val_loss: 0.0616 - val_acc: 0.9825\n",
            "Epoch 25/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0797 - acc: 0.9755 - val_loss: 0.0634 - val_acc: 0.9810\n",
            "Epoch 26/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0797 - acc: 0.9760 - val_loss: 0.0634 - val_acc: 0.9813\n",
            "Epoch 27/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0787 - acc: 0.9763 - val_loss: 0.0632 - val_acc: 0.9816\n",
            "Epoch 28/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0765 - acc: 0.9764 - val_loss: 0.0643 - val_acc: 0.9809\n",
            "Epoch 29/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0742 - acc: 0.9774 - val_loss: 0.0648 - val_acc: 0.9809\n",
            "Epoch 30/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0744 - acc: 0.9767 - val_loss: 0.0552 - val_acc: 0.9835\n",
            "Epoch 31/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0728 - acc: 0.9779 - val_loss: 0.0573 - val_acc: 0.9835\n",
            "Epoch 32/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0721 - acc: 0.9785 - val_loss: 0.0572 - val_acc: 0.9841\n",
            "Epoch 33/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0720 - acc: 0.9782 - val_loss: 0.0543 - val_acc: 0.9835\n",
            "Epoch 34/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0697 - acc: 0.9790 - val_loss: 0.0613 - val_acc: 0.9812\n",
            "Epoch 35/500\n",
            "60000/60000 [==============================] - 13s 213us/step - loss: 0.0693 - acc: 0.9788 - val_loss: 0.0621 - val_acc: 0.9825\n",
            "Epoch 36/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0690 - acc: 0.9787 - val_loss: 0.0539 - val_acc: 0.9842\n",
            "Epoch 37/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0669 - acc: 0.9795 - val_loss: 0.0573 - val_acc: 0.9829\n",
            "Epoch 38/500\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0662 - acc: 0.9797 - val_loss: 0.0500 - val_acc: 0.9845\n",
            "Epoch 39/500\n",
            "55808/60000 [==========================>...] - ETA: 0s - loss: 0.0656 - acc: 0.9795"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY334rMlgTnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_mnist()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}